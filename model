from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, StackingClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import joblib
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.models import Sequential
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.preprocessing import StandardScaler, RobustScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.preprocessing import PolynomialFeatures
from sklearn.feature_extraction.text import TfidfVectorizer

def load_data(file_path):
    """
    Load data from CSV file.

    Parameters:
    file_path (str): Path to the CSV file.

    Returns:
    pd.DataFrame: Loaded DataFrame.
    """
    return pd.read_csv(file_path)

def preprocess_data(data, numeric_features, categorical_features, text_feature, target_column):
    """
    Preprocess the raw data.

    Parameters:
    data (pd.DataFrame): Input DataFrame.
    numeric_features (list): List of numeric feature column names.
    categorical_features (list): List of categorical feature column names.
    text_feature (str): Name of the text feature column.
    target_column (str): Name of the target column.

    Returns:
    pd.DataFrame: Preprocessed DataFrame.
    pd.Series: Target variable.
    """
    # Separate features and target variable
    X = data.drop(columns=[target_column])
    y = data[target_column]
    
    # Define preprocessing steps for numeric features
    numeric_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler())])
    
    # Define preprocessing steps for categorical features
    categorical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
        ('encoder', OneHotEncoder(handle_unknown='ignore'))])
    
    # Define preprocessing steps for text feature
    text_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='constant', fill_value='')),
        ('tfidf', TfidfVectorizer())])
    
    # Combine preprocessing steps
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numeric_transformer, numeric_features),
            ('cat', categorical_transformer, categorical_features),
            ('text', text_transformer, [text_feature])])
    
    # Apply preprocessing
    preprocessed_data = preprocessor.fit_transform(X)
    
    return preprocessed_data, y

def feature_selection(X, y, num_features):
    """
    Perform feature selection using SelectKBest.

    Parameters:
    X (pd.DataFrame): Input features.
    y (pd.Series): Target variable.
    num_features (int): Number of top features to select.

    Returns:
    pd.DataFrame: Selected features.
    """
    selector = SelectKBest(score_func=f_classif, k=num_features)
    X_selected = selector.fit_transform(X, y)
    return X_selected

def polynomial_features(X, degree):
    """
    Generate polynomial features.

    Parameters:
    X (pd.DataFrame): Input features.
    degree (int): Degree of polynomial features.

    Returns:
    pd.DataFrame: Transformed features with polynomial features.
    """
    poly = PolynomialFeatures(degree=degree, include_bias=False)
    X_poly = poly.fit_transform(X)
    return X_poly

def train_model(X_train, y_train):
    """
    Train the machine learning model using advanced ensemble learning techniques.

    Parameters:
    X_train (pd.DataFrame): Features of the training set.
    y_train (pd.Series): Target variable of the training set.

    Returns:
    VotingClassifier: Trained ensemble model.
    """
    # Initialize base estimators
    random_forest = RandomForestClassifier(n_estimators=100, random_state=42)
    gradient_boosting = GradientBoostingClassifier()
    svc = SVC(probability=True)
    
    # Define the ensemble classifiers
    estimators = [
        ('random_forest', random_forest),
        ('gradient_boosting', gradient_boosting),
        ('svc', svc)
    ]
    
    # Initialize Voting Classifier
    voting_clf = VotingClassifier(estimators, voting='soft')
    
    # Train the ensemble model
    voting_clf.fit(X_train, y_train)
    
    return voting_clf

def evaluate_model(model, X_test, y_test):
    """
    Evaluate the trained machine learning model.

    Parameters:
    model (VotingClassifier): Trained ensemble model.
    X_test (pd.DataFrame): Features of the test set.
    y_test (pd.Series): Target variable of the test set.
    """
    # Make predictions
    y_pred = model.predict(X_test)
    
    # Calculate evaluation metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    
    # Print evaluation metrics
    print("Model Evaluation:")
    print(f"Accuracy: {accuracy}")
    print(f"Precision: {precision}")
    print(f"Recall: {recall}")
    print(f"F1 Score: {f1}")

def save_model(model, file_path):
    """
    Save the trained machine learning model to a file.

    Parameters:
    model (VotingClassifier): Trained ensemble model.
    file_path (str): Path to save the model file.
    """
    joblib.dump(model, file_path)
